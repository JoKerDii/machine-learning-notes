## Clustering

There are 5 topics and 4 exercises.

## 1. Basics of Clustering

A **partition** of a set is a grouping of the set's elements into non-empty subsets, in such a way that **every** element is included in one and only one of the subsets. In other words, $C_1,C_2,...,C_K$ is a partition of $\{1,2,...,n\}$ if and only if
$$
C_1 \cup C_2 \cup ... \cup C_ K = \big \{  1, 2, ..., n \big \}
$$
and
$$
C_ i \cap C_ j = \emptyset \quad \text {for any } i \neq j \text{ in } \big \{ 1, ..., k\big \} 
$$
**Input** of clustering: 

* Set of feature vectors $S_ n = \big \{ x^{(i)} | i =1,...,n \big \}$
* The number of clusters $K$
* The representatives of each cluster $z_1, ..., z_K$

**Output** of clustering

* A partition of **indices** $\big \{ 1, ..., n\big \}$ into $K$ sets, $C_1, ..., C_K$.
* "Representatives" in each of the $K$ partition sets, given as $z_1, ..., z_K$

## 2. Similarity Measures

* Cosine distance
  $$
  cos(x^{i}, x^{j}) = \frac{x^{i} \cdot x^{j}}{\|x^{i}\| \cdot \|x^{j}\|}
  $$

* Euclidean distance
  $$
  \text{dist}(x^{i}, x^{j}) = \|x^{i} - x^{j}\|^2
  $$

> #### Exercise 29
>
> If we want to measure the similarity between two Google News articles, suppose you assume that the length of an article does not tell any useful information about the article, and hence choose a similarity measure that does not depend on the length of the article. Which of the following similarity measure could be the one you chose?
>
> A. Euclidean distance
>
> B. Cosine distance
>
> > **Answer**: B
>
> > **Solution**: It can be thought that **longer articles will have larger norms**, since they are more likely to contain unique words. Because it is assumed that the length of the article does not contain any important information, it is not ideal to use the Euclidean distance.

## 3. Cost Functions

The total cost of clustering output is defined as the sum of the cost inside each cluster. 
$$
\text {Cost}(C_1, ..., C_ K) = \sum _{j=1}^{K} \text {Cost}(C_ j)
$$
where $\text {Cost}(C_ j)$ is supposed to measure "how homogeneous" the assigned data are inside the $j$th cluster $C_j$.

If $\text {Cost}(C_ j)$ is **sum of the squared Euclidean distance**, the cost the clustering output is 
$$
\text {Cost}(C_1, ..., C_ K) = \sum _{j=1}^{K} \text {Cost}(C_ j) = \sum _{j=1}^{K} \sum _{i \in C_ j} \| x_ i - z_ j\| ^2
$$

## 4. K-Means Algorithm

Given a set of features $S_ n = \big \{ x^{(i)}| i = 1,...,n\big \}$, and the number of clusters $K$, we can use the $K$-Means algorithms to find good cluster assignments $C_1, ..., C_K$ and the representatives of each of the $K$ clusters $z_1, ..., z_K$. The algorithm is:

1. Randomly select $z_1, ..., z_K$

2. Iterate through

   a. Assign each point $x^{(i)}$ to the closest $z_j$, so that
   $$
   \text {Cost}(z_1, ... z_ K) = \sum _{i=1}^{n} \min _{j=1,...,K} \left\|  x^{(i)} - z_ j \right\| ^2
   $$
   b. Given $C_ j \in \big \{ C_1,...,C_ K\big \}$ find the best representative $z_ j \in \big \{ x_1,...,x_ n\big \}$ such that
   $$
   z_ j=\operatorname {argmin}_{z} \sum _{i \in C_ j} \| x^{(i)} - z \| ^2 = \frac{\sum _{i \in C_ j} x^{(i)}}{|C_ j|}
   $$
   where $|C_j|$ is the number of points in cluster $C_j$

#### Specifically, to find the representative $z$

We compute the following gradient, set it to zero, and obtain $z_j$ that minimize the **sum of squared Euclidean distance**:
$$
\nabla _{z_ j}\left(\sum _{i \in \mathbb {C}_ j} \| x^{(i)} - z_ j\| ^2\right) = \sum _{i \in \mathbb {C}_ j} -2(x^{(i)} - z_ j)  = 0 \\
z_j = \frac{\sum _{i \in C_ j} x^{(i)}}{|C_ j|}
$$

* The value of $z_j$ is only affected by points $\{x_I : i \in C_j\}$.
* The obtained $z_j$ is the **centroid** (center of mass assuming each $x^{(i)}$ has equal mass) of the $j$th cluster. (This happens only when using sum of squared Euclidean distance.)
* Steps 2a and 2b of the algorithm always decrease the cost or keep it the same at least.

#### Impact of Initialization

The output of the algorithm largely depends upon the initialization in Step 1. Thus, in practice it is wise to 

* make sure that $z_1,...z_K$ are initialized so that they are **well spread out**. 
* try **multiple initializations** and choose the clustering output that appears the most commonly.

#### Drawbacks

* **Manual choice of $K$**. K-means does not have a built-in mechanism to choose the value of $K$ automatically.
* **Not robust to outliers**. Centroids can be dragged around by outliers or outliers might get their own cluster.
* **Does not scale well with increasing number of dimensions**. With increasing number of dimensions, a distance-based similarity measure converges to a constant value between any given examples.

> #### Exercise 30
>
> Assuming that there are n data points $\{x_1,...,x_n\}$, $K$ clusters and representatives,and each $x_i∈\{x_1,...,x_n\}$ is a vector of dimension $d$, what is the computational complexity for one complete iteration of the k-means algorithm?
>
> > **Answer**: $O(ndK)$

## 5. K-Medoids Algorithm

1. Randomly select $\big \{  z_1, ..., z_ K \big \}  \subseteq \big \{  x_1, ..., x_ n \big \}$

2. Iterate

   a. Assign each $x^{(i)}$ to the closest $z_j$, so that
   $$
   \text {Cost}(z_1, ... z_ K) = \sum _{i=1}^{n} \min _{j=1,...,k} \text {dist}(x^{(i)}, z_ j)
   $$
   b. Given $C_ j \in \big \{ C_1,...,C_ K\big \}$ find the best representative $z_ j \in \big \{ x_1,...,x_ n\big \}$ such that
   $$
   z_j =  \operatorname {argmin} \sum _{x^{(i)} \in C_ j} \text {dist}(x^{(i)}, z_ j)
   $$

Two limitations of K-Means are resolved now: 

* The $K$ representatives $z_1, ..., z_ K \in \big \{ x_1, ..., x_ n \big \}$, which would give meaningful representatives and address some visualization concerns in certain fields such as NLP.
* The step 2b finds the best representatives for any given distance measure.

> #### Exercise 31
>
> For the K-Medoids algorithm, what is the complexity of step 2a? What is the complexity of step 2b?
>
> > **Answer**: $O(ndK)$ and $O(n^2dK)$.
>
> > **Solution**: 
> >
> > Note that step 2a of the K-Medoids is the same as that of K-Means, while step 2b of K-Medoids has an additional loop of iterating through the $n$ points $z_ j \in \big \{ x_1,...,x_ n\big \}$ which takes $O(n)$.

> #### Exercise 32
>
> There are some elements that we can supervise in unsupervised learning. Which of the following are elements of clustering that we can and should tune, depending on the application?
>
> > **Answer**: 
> >
> > The **number of clusters** $K$ and the **cost measure for distance** between $x^{(i)}∈C_j$ and $z_j (dist(x^{(i)},z_j))$ are what we can control/ supervise.



