# Collaborative Filtering via Gaussian Mixtures

This is a little bit recap / summary of GMM and EM, and hints / ideas for project 4 which is an application case on incomplete data.

## 1. Data Generation Model

The Generation Model is 

1. Given inputs $x^{(1)},x^{(2)},...,x^{(n)} \in \R^d$
2. Pick $K$ mixture components each with probability $p_1, p_2, ..., p_k$, and $\sum^k_{j=1}p_j = 1$. Each generates from a multinomial Gaussian distribution $j \sim \text{Multinomial}(p_1, ..., p_k)$.
3. Generate points $x \in \R^d$ and $x \sim P(x | \mu^{(j)},\sigma_j^2)$, where $\mu^{(j)} \in \R^d$ and $\sigma_j^2 = \sigma_j^2 \begin{bmatrix} 1 & & &\\ &1 & & \\ & &...& \\& & & 1\end{bmatrix}_{d \times d}$. (Note that the covariance matrix $\sigma_j^2$ can be generalized to not be diagonal matrix with every value the same)
4. The output is estimates of $\Theta = \{p_1, ..., p_K, \mu^{(1)}, ..., \mu^{(K)}, \sigma_1^2, ..., \sigma_K^2\}$, which is the goal of **EM** algorithm.

Note that the data generation model can be used for soft classification or hard classification. The distribution may not necessarily be assumed to be Gaussian, it can be any other kinds of distribution.

## 2. EM Algorithm

1. Randomly initialize $\hat{\Theta}$. Compute likelihood $L_{old} = \prod^n_{i=1}p(x^{(i)}|\theta)$.

2. E-step: Update $p(j|i) = \mathbb{P}[\text{j generated }x^{(i)} |x^{(i)}, \Theta]$, where $j = 1,..., K, i = 1, ..., n$.

3. M-step: Minimize the function
   $$
   \hat{l}(x^{(i)}, ..., x^{(n)}| \Theta) = \sum^n_{i = 1} \sum^K_{j = 1} p(j|i) \text{log} (\frac{\mathbb{P}[\text{j generated }x^{(i)} |\Theta]}{p(j|i)})
   $$
   where $\mathbb{P}[\text{j generated }x^{(i)} |\Theta]$ is the likelihood of $x^{(i)}$ generated by cluster $j$. 
   $$
   \mathbb{P}[\text{j generated }x^{(i)} | \Theta] = p_j \cdot p(x^{(i)}| \mu^{(j)}, \sigma_j^2I)
   $$

4.  Take the derivative of $\hat{l}$ with respect to $\mu^{(j)}, \sigma^2_j, p_j$, for $j = 1, ..., K$.  Set to zero and solve for $\widehat{\mu^{(j)}}, \widehat{\sigma^2_j}, \widehat{p_j}$
   $$
   \begin{aligned}
   \widehat{\mu ^{(j)}} &= \frac{\sum _{i = 1}^ n p (j \mid i) \mathbf x^{(i)}}{\sum _{i=1}^ n p(j \mid i)}\\
   \widehat{p_ j} &= \frac{1}{n}\sum _{i = 1}^ n p(j \mid i)\\
   \widehat{\sigma _ j^2} &= \frac{\sum _{i = 1}^ n p(j \mid i) \|  \mathbf x^{(i)} - \widehat{\mu ^{(j)}} \| ^2}{d \sum _{i = 1}^ n p(j \mid i)}
   \end{aligned}
   $$

5. The convergence criteria: If $|L_{new} - L_{old}| < \epsilon$ (usually $epsilon = 10^{-6}$), stop, else go to (2) step.

Note that EM should **monotonically increase** (non-decreasing) the log-likelihood of the data.

## 3. GMM for Matrix Completion

In the movie rating problem, the input matrix $X$ is of size $n \times d$, with entries $1,2,3,4,5$ if the ratings are observed, or $0$ if the rating is missing. The rows of $X$ are users $U = 1,..., n \rightarrow x^{(u)}: \text{ rows of } X $. The columns of $X$ are $0, ..., d-1$ movies. To take missing ratings into account, we define $C_u = \{\text{ columns indexes of observed ratings}\}$ as the set of movies that user $u$ has rated $H_u$ as its complement $H_u = \{\text{ columns indexes of unobserved ratings}\}$.

For example, when there are 5 movies and 3 users
$$
\begin{aligned}
X &= \begin{bmatrix} 1&2&0&1&3 \\ 5&0&0&2&4 \\ 1&2&5&0&1 \end{bmatrix}\\
C_1 &= \{1,2,4,5\}\\
C_2 &= \{1,4,5\}\\
C_3 &= \{1,2,3,5\}
\end{aligned}
$$
The model for generating $x^{(1)},x^{(2)},...,x^{(n)} \in \R^d$ with $K$ components and estimating $\Theta = \{p_1, ..., p_K, \mu^{(1)}, ..., \mu^{(K)}, \sigma_1^2, ..., \sigma_K^2\}$.

The mixture model for a complete rating vector is written as:
$$
P(x^{(u)}| \Theta) = \sum^K_{j=1} p_j \mathcal{N}(x^{(u)}; \mu^{(j)}, \sigma_j^2I)
$$

**In the presence of missing values:**

We must use the **marginal probability** $p(x^{(u)}_{C_u}| \Theta)$ that is over only the observed values. This marginal corresponds to integrating the mixture density $P(x^{(u)}| \theta)$ over all the unobserved coordinate values. It can be computed as follows. We first decompose the multivariate spherical Gaussian as a product of univariate Gaussians (since there is no covariance between coordinates)
$$
\begin{aligned}
P(x^{(u)} | \theta ) &= \sum ^{K}_{j=1} p_ j \prod _ i N(x_ i^{(u)}; \mu _ i^{(j)}, \sigma _ i^{2,(j)})\\
&=  \sum ^{K}_{j=1} p_ j \prod _{m \in C_ u} N(x_ m^{(u)}; \mu _ m^{(j)}, \sigma _ m^{2,(j)}) \prod _{m' \in H_ u} N(x_{m'}^{(u)}; \mu _{m'}^{(j)}, \sigma _{m'}^{2,(j)})
\end{aligned}
$$
For $m' \in H_u$ we can marginalize over all of the unobserved values to get 
$$
\int N(x_{m'}^{(u)}; \mu _{m'}^{(j)}, \sigma _{m'}^{2,(j)}) dx_{m'}^{(u)} = 1
$$
Thus, the mixture density can be written as
$$
P(x^{(u)}_{C_u}| \Theta) = \sum^K_{j=1}p_j \mathcal{N}(x^{(u)}_{C_u}; \mu^{(j)}_{C_u}, \sigma_j^2I_{|C_u| \times |C_u|})
$$
where $I_{|C_u| \times |C_u|}$ is the identity matrix in $|C_u|$ dimensions.

**Main idea** here is we can assign the mean $\widehat{\mu_l^{(s)}}$ to each user unobserved entries $l$ if we determine the user belongs to cluster $S$. 

**Trick** for numerical stability: Assume we wish to evaluate $y=\log (\exp (x_{1})+...\exp (x_{n}))$, we define $x^*=\max \{ x_{1}..x_{n}\}$. Then, 
$$
y=x^*+\log (\exp (x_{1}-x^*)+...\exp (x_{n}-x^*))
$$


## 4. EM Algorithm for Matrix Completion

1. Randomly initialize $\widehat{\Theta} = \{\widehat{p_1} ... \widehat{p_K},\widehat{\sigma^2_1}...\widehat{\sigma^2_K}, \widehat{\mu^2_1}...\widehat{\mu^2_K}\}$.

2. E step: 
   $$
   p(j|u) = \frac{p(x^{(u)} | j, \Theta) \cdot p_j}{\sum^K_{j=1}p(x^{(u)} | j, \Theta) \cdot p_j} = \frac{p_j \mathcal{N}(x^{(u)}_{C_u}; \mu_{C_u}, \sigma_j^2I_{|C_u| \times |C_u|})}{\sum^K_{j=1}p(x^{(u)} | j, \Theta) \cdot p_j}
   $$
   where $j = 1...K$ and $u = 1...n$.

3. M step: 
   $$
   \widehat{l}(x;\Theta) = \sum^n_{u=1} \sum^K_{j=1}p(j|u) \log (\frac{p(j \text{ generated } x^{u}| \Theta)}{p(j|u)})
   $$

**Main idea**: 

For $j = 1...K$, we estimate $\widehat{\mu}^{(j)} = \begin{bmatrix} \widehat{\mu_1^{(j)}} \\ ... \\ \widehat{\mu_d^{(j)}} \\ \end{bmatrix}$ and $l$ is the index of a $\mu_l^{(j)}$. The index $l$ affects $p(x_{C_u}^{(j)}| \mu_{C_u}^{(j)}, \sigma_j^2 I_{|C_u| \times |C_u|})$ only if $l \in C_u$.
$$
\begin{aligned}
\frac{\partial \widehat{l}(x; \Theta)}{\partial \mu_l^{(j)}} &= 0 \text{ where } i=1...K, l = 0...d-1\\
\frac{\partial \widehat{l}}{\partial \sigma^2_j} &= 0 \text{ where } j = 1...K\\
\frac{\partial \widehat{l}}{\partial p_j} &= 0 \text{ where } j = 1...K
\end{aligned}
$$

**In the presence of missing values**:

We use Bayes' Rule to find an updated expression for the posterior probability $p(j|u) = P(y=j|x_{C_u}^{(u)})$. This is a **soft assignment** of cluster $j$ to data point $u$.
$$
p(j\mid u) =\frac{p(u|j)\cdot p(j)}{p(u)} =\frac{p(u|j)\cdot p(j)}{\sum _{j=1}^{K}p(u|j)\cdot p(j)} =\frac{ \pi _{j}N(x_{C_{u}}^{(u)};\mu _{C_{u}}^{(j)},\sigma _{j}^{2}I_{C_{u}\times C_{u}}) }{ \sum _{j=1}^{K}\pi _{j}N(x_{C_{u}}^{(u)};\mu _{C_{u}}^{(j)},\sigma _{j}^{2}I_{C_{u}\times C_{u}}) }
$$
Then we calculate the log of posterior probability $\ell (j,u)=\log (p(j|u))$ to minimize numerical instability, though the actual output of **E-step** should include the non-log posterior.

Let $f(u,i)=\log (\pi _{i})+\log \left(N(x_{C_{u}}^{(u)};\mu _{C_{u}}^{(i)},\sigma _{i}^{2}I_{C_{u}\times C_{u}})\right)$, the log posterior is 
$$
\begin{aligned}
\ell (j|u)  &= \log (p(j\mid u)) \\&= \log \left(\frac{ \pi _{j}N(x_{C_{u}}^{(u)};\mu _{C_{u}}^{(j)},\sigma _{j}^{2}I_{C_{u}\times C_{u}}) }{ \sum _{j=1}^{K}\pi _{j}N(x_{C_{u}}^{(u)};\mu _{C_{u}}^{(j)},\sigma _{j}^{2}I_{C_{u}\times C_{u}}) }\right)\\
 &= \log \left(\pi _{j}N(x_{C_{u}}^{(u)};\mu _{C_{u}}^{(j)},\sigma _{j}^{2}I_{C_{u}\times C_{u}})\right) - \log \left(\sum _{j=1}^{K}\pi _{j}N(x_{C_{u}}^{(u)};\mu _{C_{u}}^{(j)},\sigma _{j}^{2}I_{C_{u}\times C_{u}})\right)\\
 &=\log (\pi _{j})+\log \left(N(x_{C_{u}}^{(u)};\mu _{C_{u}}^{(j)},\sigma _{j}^{2}I_{C_{u}\times C_{u}})\right) - \log \left(\sum _{j=1}^{K}\exp (\log (\pi _{j}N(x_{C_{u}}^{(u)};\mu _{C_{u}}^{(j)},\sigma _{j}^{2}I_{C_{u}\times C_{u}})))\right)\\
 &= f(u,j)-\log \left(\sum _{j=1}^{K}\exp (f(u,j))\right)
 \end{aligned}
$$
In **M-step**, we wish to find the parameters $\pi, \mu, $ and $\sigma$ that maximize $\ell (X;\theta )$.

We **decompose the multivariate spherical Gaussians into univariate spherical Gaussians** and get, if $l \in C_u$,
$$
\frac{\partial }{\partial \mu _ l^{(k)}} N(x_{C_{u}}^{(u)}|\mu _{C_{u}}^{(k)},\sigma _{k}^{2}I_{|C_{u}|\times |C_{u}|}) = N(\dots ) \frac{ \frac{\partial }{\partial \mu _ l^{(k)}} \left(\frac{1}{\sqrt{2\pi } \sigma _{l,(k)}} \exp (-\frac{1}{2\sigma _{l,(k)}^2} (x^{(u)}_ l - \mu _ l^{(k)})^2 ) \right) }{ \left(\frac{1}{\sqrt{2\pi } \sigma _{l,(k)}} \exp (-\frac{1}{2\sigma _{l,(k)}^2} (x^{(u)}_ l - \mu _ l^{(k)})^2 ) \right) }\\
= N(\dots ) \frac{x_ l^{(u)} - \mu _ l^{(k)}}{\sigma _{l,(k)}^2}
$$
where $N(\dots ) = N(x_{C_{u}}^{(u)}|\mu _{C_{u}}^{(k)},\sigma _{k}^{2}I_{|C_{u}|\times |C_{u}|})$. Otherwise, if $l \notin C_u$, those derivatives are 0. 

Thus the complete derivative equation could be written as
$$
\frac{\partial }{\partial \mu _ l^{(k)}} N(x_{C_{u}}^{(u)}|\mu _{C_{u}}^{(k)},\sigma _{k}^{2}I_{|C_{u}|\times |C_{u}|}) = N(x_{C_{u}}^{(u)}|\mu _{C_{u}}^{(k)},\sigma _{k}^{2}I_{|C_{u}|\times |C_{u}|}) \delta (l,C_ u) \frac{x_ l^{(u)} - \mu _ l^{(k)}}{\sigma _{l,(k)}^2}
$$
where $\delta (i,C_{u})$ is an indicator function.

The proxy likelihood function $\hat{\ell }(X ; \theta )$ that we want to maximize is 
$$
\begin{aligned}
\hat{\ell }(X;\theta ) &=\sum _{u = 1}^ n \sum _{j = 1}^ K p(j \mid u) \log \left(\frac{p\left( x^{(u)} \text { generated by cluster } j ; \theta \right)}{p(j \mid u)}\right)\\
&= \sum _{u = 1}^ n \sum _{j = 1}^ K p(j \mid u) \log \left(\frac{\pi _ j \mathcal{N}(x_{C_ u}^{(u)} \mid \mu _{C_ u}^{(j)}, \sigma _ j^2 I_{|C_ u| \times |C_ u|})}{p(j \mid u)}\right)
\end{aligned}
$$
where $p\left( x^{(u)} \text { generated by cluster } j ; \theta \right)$ is the likelihood of $x^{(u)}$ generated by cluster $j$ and the parameter set is $\theta$. $p(j \mid u)$ was computed in E-step as constants for M-step.

Now take the partial derivative with respect to $\mu_l^{(k)}$ , set it to zero and solve for $\widehat{\mu_l^{(k)}}$.
$$
\begin{aligned}
\frac{\partial \hat{\ell }(X;\theta )}{\partial \mu _ l^{(k)}} &= - \frac{\partial }{\partial \mu _ l^{(k)}} \left[\sum _{u = 1}^ n \sum _{j = 1}^ K p(j \mid u) \cdot \frac{1}{2} \cdot \frac{\|  x_{C_ u}^{(u)} - \mu _{C_ u}^{(j)}\| ^2}{\sigma _ j^2} \right]\\
 &=\sum _{u = 1}^ n p(k \mid u) \delta (l,C_ u) \frac{x_ l ^{(u)} - \mu _ l^{(k)}}{\sigma _ k^2}\\
 \widehat{\mu _ l^{(k)}} &= \frac{\sum _{u = 1}^ n p(k \mid u) \delta (l,C_ u) x_ l^{(u)}}{\sum _{u=1}^ n p(k \mid u) \delta (l,C_ u)}
\end{aligned}
$$
Similarly we solve for $\widehat{\sigma _ k^2}$ and $\widehat{\pi _ k}$
$$
\begin{aligned}
\widehat{\sigma _ k^2} &= \frac{1}{\sum _{u=1}^ n |C_ u| p(k \mid u)} \sum _{u = 1}^ n p(k \mid u) \| x_{C_ u}^{(u)} - \widehat{\mu _{C_ u}^{(k)}}\| ^2\\
\widehat{\pi _ k} &= \frac{1}{n} \sum _{u=1}^ n p(k \mid u)
\end{aligned}
$$


## 5. Bayesian Information Criterion (BIC)

The **Bayesian information criterion (BIC)** is a criterion for model selection. It captures the tradeoff between the log-likelihood of the data, and the number of parameters that the model uses. The BIC of a model M is defined as:
$$
\text {BIC}(M) = l - \frac{1}{2}p \log n
$$
where $l$ is the log-likelihood of the data under the current model (highest log-likelihood we can achieve by adjusting the parameters in the model), $p$ is the number of free parameters, and $n$ is the number of data points. This score rewards a larger log-likelihood, but penalizes the number of parameters used to train the model. In a situation where we wish to select models, we want a model with the the highest BIC.

