# Collaborative Filtering via Gaussian Mixtures

This is a little bit recap / summary of GMM and EM, and hints / ideas for project 4.

## 1. Data Generation Model

The Generation Model is 

1. Given inputs $x^{(1)},x^{(2)},...,x^{(n)} \in \R^d$
2. Pick $K$ mixture components each with probability $p_1, p_2, ..., p_k$, and $\sum^k_{j=1}p_j = 1$. Each generates from a multinomial Gaussian distribution $j \sim \text{Multinomial}(p_1, ..., p_k)$.
3. Generate points $x \in \R^d$ and $x \sim P(x | \mu^{(j)},\sigma_j^2)$, where $\mu^{(j)} \in \R^d$ and $\sigma_j^2 = \sigma_j^2 \begin{bmatrix} 1 & & &\\ &1 & & \\ & &...& \\& & & 1\end{bmatrix}_{d \times d}$. (Note that the covariance matrix $\sigma_j^2$ can be generalized to not be diagonal matrix with every value the same)
4. The output is estimates of $\Theta = \{p_1, ..., p_K, \mu^{(1)}, ..., \mu^{(K)}, \sigma_1^2, ..., \sigma_K^2\}$, which is the goal of **EM** algorithm.

Note that the data generation model can be used for soft classification or hard classification. The distribution may not necessarily be assumed to be Gaussian, it can be any other kinds of distribution.

## 2. EM Algorithm

1. Randomly initialize $\hat{\Theta}$. Compute likelihood $L_{old} = \prod^n_{i=1}p(x^{(i)}|\theta)$.

2. E-step: Update $p(j|i) = \mathbb{P}[\text{j generated }x^{(i)} |x^{(i)}, \Theta]$, where $j = 1,..., K, i = 1, ..., n$.

3. M-step: Minimize the function
   $$
   \hat{l}(x^{(i)}, ..., x^{(n)}| \Theta) = \sum^n_{i = 1} \sum^K_{j = 1} p(j|i) \text{log} (\frac{\mathbb{P}[\text{j generated }x^{(i)} |\Theta]}{p(j|i)})
   $$
   where $\mathbb{P}[\text{j generated }x^{(i)} |\Theta]$ is the likelihood of $x^{(i)}$ generated by cluster $j$. 
   $$
   \mathbb{P}[\text{j generated }x^{(i)} | \Theta] = p_j \cdot p(x^{(i)}| \mu^{(j)}, \sigma_j^2I)
   $$

4.  Take the derivative of $\hat{l}$ with respect to $\mu^{(j)}, \sigma^2_j, p_j$, for $j = 1, ..., K$.  Set to zero and solve for $\widehat{\mu^{(j)}}, \widehat{\sigma^2_j}, \widehat{p_j}$
   $$
   \begin{aligned}
   \widehat{\mu ^{(j)}} &= \frac{\sum _{i = 1}^ n p (j \mid i) \mathbf x^{(i)}}{\sum _{i=1}^ n p(j \mid i)}\\
   \widehat{p_ j} &= \frac{1}{n}\sum _{i = 1}^ n p(j \mid i)\\
   \widehat{\sigma _ j^2} &= \frac{\sum _{i = 1}^ n p(j \mid i) \|  \mathbf x^{(i)} - \widehat{\mu ^{(j)}} \| ^2}{d \sum _{i = 1}^ n p(j \mid i)}
   \end{aligned}
   $$

5. The convergence criteria: If $|L_{new} - L_{old}| < \epsilon$ (usually $epsilon = 10^{-6}$), stop, else go to (2) step.

Note that EM should **monotonically increase** (non-decreasing) the log-likelihood of the data.

## 3. GMM for Matrix Completion

In the movie rating problem, the input matrix $X$ is of size $n \times d$, with entries $1,2,3,4,5$ if the ratings are observed, or $0$ if the rating is missing. The rows of $X$ are users $U = 1,..., n \rightarrow x^{(u)}: \text{ rows of } X $. The columns of $X$ are $0, ..., d-1$ movies. To take missing ratings into account, we define $C_u = \{\text{ columns indexes of observed ratings}\}$ as the set of movies that user $u$ has rated $H_u$ as its complement $H_u = \{\text{ columns indexes of unobserved ratings}\}$.

For example, when there are 5 movies and 3 users
$$
\begin{aligned}
X &= \begin{bmatrix} 1&2&0&1&3 \\ 5&0&0&2&4 \\ 1&2&5&0&1 \end{bmatrix}\\
C_1 &= \{1,2,4,5\}\\
C_2 &= \{1,4,5\}\\
C_3 &= \{1,2,3,5\}
\end{aligned}
$$
The model for generating $x^{(1)},x^{(2)},...,x^{(n)} \in \R^d$ with $K$ components and estimating $\Theta = \{p_1, ..., p_K, \mu^{(1)}, ..., \mu^{(K)}, \sigma_1^2, ..., \sigma_K^2\}$.

The mixture model for a complete rating vector is written as:
$$
P(x^{(u)}| \Theta) = \sum^K_{j=1} p_j \mathcal{N}(x^{(u)}; \mu^{(j)}, \sigma_j^2I)
$$

In the presence of missing values, we must use the **marginal probability** $p(x^{(u)}_{C_u}| \Theta)$ that is over only the observed values. This marginal corresponds to integrating the mixture density $P(x^{(u)}| \theta)$ over all the unobserved coordinate values. It can be computed as follows. We first decompose the multivariate spherical Gaussian as a product of univariate Gaussians (since there is no covariance between coordinates)
$$
\begin{aligned}
P(x^{(u)} | \theta ) &= \sum ^{K}_{j=1} p_ j \prod _ i N(x_ i^{(u)}; \mu _ i^{(j)}, \sigma _ i^{2,(j)})\\
&=  \sum ^{K}_{j=1} p_ j \prod _{m \in C_ u} N(x_ m^{(u)}; \mu _ m^{(j)}, \sigma _ m^{2,(j)}) \prod _{m' \in H_ u} N(x_{m'}^{(u)}; \mu _{m'}^{(j)}, \sigma _{m'}^{2,(j)})
\end{aligned}
$$
For $m' \in H_u$ we can marginalize over all of the unobserved values to get 
$$
\int N(x_{m'}^{(u)}; \mu _{m'}^{(j)}, \sigma _{m'}^{2,(j)}) dx_{m'}^{(u)} = 1
$$
Thus, the mixture density can be written as
$$
P(x^{(u)}_{C_u}| \Theta) = \sum^K_{j=1}p_j \mathcal{N}(x^{(u)}_{C_u}; \mu^{(j)}_{C_u}, \sigma_j^2I_{|C_u| \times |C_u|})
$$
where $I_{|C_u| \times |C_u|}$ is the identity matrix in $|C_u|$ dimensions.

**Main idea** here is we can assign the mean $\widehat{\mu_l^{(s)}}$ to each user unobserved entries $l$ if we determine the user belongs to cluster $S$. 

**Trick** for numerical stability: Assume we wish to evaluate $y=\log (\exp (x_{1})+...\exp (x_{n}))$, we define $x^*=\max \{ x_{1}..x_{n}\}$. Then, 
$$
y=x^*+\log (\exp (x_{1}-x^*)+...\exp (x_{n}-x^*))
$$


## 4. EM Algorithm for Matrix Completion

1. Randomly initialize $\widehat{\Theta} = \{\widehat{p_1} ... \widehat{p_K},\widehat{\sigma^2_1}...\widehat{\sigma^2_K}, \widehat{\mu^2_1}...\widehat{\mu^2_K}\}$.

2. E step: 
   $$
   p(j|u) = \frac{p(x^{(u)} | j, \Theta) \cdot p_j}{\sum^K_{j=1}p(x^{(u)} | j, \Theta) \cdot p_j} = \frac{p_j \mathcal{N}(x^{(u)}_{C_u}; \mu_{C_u}, \sigma_j^2I_{|C_u| \times |C_u|})}{\sum^K_{j=1}p(x^{(u)} | j, \Theta) \cdot p_j}
   $$
   where $j = 1...K$ and $u = 1...n$.

3. M step: 
   $$
   \widehat{l}(x;\Theta) = \sum^n_{u=1} \sum^K_{j=1}p(j|u) \log (\frac{p(j \text{ generated } x^{u}| \Theta)}{p(j|u)})
   $$

**Main idea**: 

For $j = 1...K$, we estimate $\widehat{\mu}^{(j)} = \begin{bmatrix} \widehat{\mu_1^{(j)}} \\ ... \\ \widehat{\mu_d^{(j)}} \\ \end{bmatrix}$ and $l$ is the index of a $\mu_l^{(j)}$. The index $l$ affects $p(x_{C_u}^{(j)}| \mu_{C_u}^{(j)}, \sigma_j^2 I_{|C_u| \times |C_u|})$ only if $l \in C_u$.
$$
\begin{aligned}
\frac{\partial \widehat{l}(x; \Theta)}{\partial \mu_l^{(j)}} &= 0 \text{ where } i=1...K, l = 0...d-1\\
\frac{\partial \widehat{l}}{\partial \sigma^2_j} &= 0 \text{ where } j = 1...K\\
\frac{\partial \widehat{l}}{\partial p_j} &= 0 \text{ where } j = 1...K
\end{aligned}
$$

## 5. Bayesian Information Criterion (BIC)

The **Bayesian information criterion (BIC)** is a criterion for model selection. It captures the tradeoff between the log-likelihood of the data, and the number of parameters that the model uses. The BIC of a model M is defined as:
$$
\text {BIC}(M) = l - \frac{1}{2}p \log n
$$
where $l$ is the log-likelihood of the data under the current model (highest log-likelihood we can achieve by adjusting the parameters in the model), $p$ is the number of free parameters, and $n$ is the number of data points. This score rewards a larger log-likelihood, but penalizes the number of parameters used to train the model. In a situation where we wish to select models, we want a model with the the highest BIC.

